{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvanJ-02/Intelligence_Artificielle_B1IM/blob/main/3_Traitement_du_langage_naturel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL2G9ul_HEi_"
      },
      "source": [
        "# Traitement du langage naturel\n",
        "\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "### 1.1. Qu'est-ce que le traitement automatique du langage naturel ?\n",
        "\n",
        "Le traitement du langage naturel (NLP en anglais pour natural language processing) est un champ de l'informatique, de l'intelligence artificielle et de la linguistique qui traite des interactions entre les ordinateurs et les langues humaines (naturelles). Le but du NLP est de permettre aux ordinateurs de comprendre, d'interpr√©ter et de g√©n√©rer des langues humaines.\n",
        "\n",
        "Les applications du NLP incluent :\n",
        "- la classification de textes, \n",
        "- l'analyse de sentiments, \n",
        "- la traduction de langues,\n",
        "- la correction grammaticale \n",
        "- la reconnaissance d'entit√©s nomm√©es, \n",
        "- la reconnaissance vocale,\n",
        "- les chatbots.\n",
        "\n",
        "\n",
        "Nous avons de plus en plus l'habitude d'√™tre en interaction avec cette branche de l'IA notamment via :\n",
        "\n",
        "üëâüèª **les outils de correction en ligne tels que [Scribens](https://www.scribens.fr/), [reverso](https://www.reverso.net/orthographe/correcteur-francais/) ou bien les correcteurs orthographiques des messageries t√©l√©phoniques**,\n",
        "\n",
        "<img src='https://www.barometre-entreprendre.fr/wp-content/uploads/2022/05/les-avantages-de-scribens.png'>\n",
        "\n",
        "üëâüèª **les propositions de r√©ponse automatique telle que le propose Google dans son outil de messagerie Gmail**,\n",
        "<img src='https://legeekducerisier.fr/wp-content/uploads/reponses-suggerees-gmail.png'>\n",
        "\n",
        "üëâüèª **les applications de traduction tel que Google Translate**,\n",
        "\n",
        "<img src='https://lh3.googleusercontent.com/EEZl-eNxoEik6MTLsK9BFtfKrsNVOy7lnNq3DS4Db9Qn3l9F68gNZHvrqeHFeLB-_d8qi9a0ZgYjYh3MCvB3mB0uh-oH0F5IYyYYC5grS1iGUd1z7oLuXV0dqMV9CuWSvV3OUPwz'>\n",
        "\n",
        "üëâüèª **les chatbots de relation client pr√©sents sur certains sites internet**,\n",
        "\n",
        "<img src='https://offreduweb.com/wp-content/uploads/5561/chatbot-le-robot-5f02378e4438b.png'>\n",
        "\n",
        "\n",
        "üëâüèª **les assistants vocaux tels qu'Alexa, Cortana, Google**,\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1MBtYT8Rd-ga18bwcQqbqI1atmC7HhkgD'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Les techniques de NLP utilisent des algorithmes d'apprentissage automatique, \n",
        "- tels que les arbres de d√©cision, \n",
        "- les for√™ts al√©atoires, \n",
        "- les r√©seaux de neurones et l'apprentissage profond, pour analyser et mod√©liser la structure et la signification des langues.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Les bases de python √† conna√Ætre pour faire du NLP\n",
        "\n",
        "<img src='https://miro.medium.com/max/1200/1*Pl9OyXXrLH_5JiQB8vNx3w.jpeg'>\n",
        "\n",
        "La premi√®re partie de cette formation sera ce consentrera sur les fondamentaux du NLP ainsi que les bases √† connaitre en python pour manipuler du contenu textuel.\n",
        "\n",
        "\n",
        "### 2.1 Les fonctions int√©gr√©es\n",
        "\n",
        "- `print()` : permet d'afficher du texte ou des valeurs sur la sortie standard, g√©n√©ralement la console.\n",
        "- `input()` : lit une entr√©e de l'utilisateur et la retourne sous forme de cha√Æne de caract√®res.\n",
        "- `len()` : cette fonction retourne la longueur d'un objet, comme une liste, un tableau ou une cha√Æne de caract√®res.\n",
        "- `type()` : cette fonction retourne le type d'un objet.\n",
        "- `dir()` : retourne une liste des noms d'attributs et de m√©thodes associ√©s √† un objet donn√©. \n",
        "- `range()` :  retourne une s√©quence d'entiers, g√©n√©r√©e en fonction d'un d√©but, d'une fin et d'un pas donn√©s.\n",
        "- `str()`, `int()`,`float()`,`bool()` , `list()`, `dict()`, `tuple()` : **data type** permettant √©galement des objets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cErOu0ZjHEjB",
        "outputId": "0d0c37f0-da0b-4b7a-ff88-e83584ea59fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__name__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__qualname__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__self__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__text_signature__']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "dir(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QBn3QeBHEjC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s_28ehbHEjC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMFOQt9LHEjD"
      },
      "source": [
        "### 2.2 Liste compr√©hension et conditions\n",
        "\n",
        "Une liste compr√©hension est une mani√®re concise et lisible  de cr√©er des listes √† partir d'autres listes. Une liste compr√©hension est √©crite en utilisant une expression simplifi√©e qui d√©crit comment les √©l√©ments doivent √™tre transform√©s. Cela permet de g√©n√©rer des listes plus rapidement et avec moins de code qu'en utilisant des boucles classiques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TAeRJvF0HEjD",
        "outputId": "7889fbda-6bbe-4f0f-d8a4-51337dd33920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 4, 16, 36, 64, 100]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Cr√©ation d'une liste de mani√®re traditionnelle avec une boucle for\n",
        "l_carre = []\n",
        "for a in range(11):\n",
        "    if a %2 ==0:\n",
        "        l_carre.append(a**2)\n",
        "\n",
        "l_carre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lucdTGk3HEjE",
        "outputId": "8e05d079-49d9-4584-eccc-d8557604f582",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 4, 16, 36, 64, 100]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Liste compr√©hension : Carr√© des nombres paires jusqu'√† 10\n",
        "[a**2 for a in range(11) if a %2 ==0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTZqt5SRHEjF"
      },
      "source": [
        "\n",
        "### 2.3 Programation ori√©t√©e Objet\n",
        "\n",
        "La programmation orient√©e objet en Python est un paradigme de programmation qui utilise des objets et des classes pour structurer le code. \n",
        "\n",
        "Les objets sont des **instances de classes**, possedant des atributs et des m√©thodes. Les attributs sont des variables de classe, tandis que les m√©thodes sont des fonctions de classe. \n",
        "\n",
        "Les classes peuvent h√©riter les propri√©t√©s et les m√©thodes d'autres classes, ce qui permet de cr√©er des relations de parent√© entre les objets. En utilisant la programmation orient√©e objet, on peut cr√©er des programmes plus organis√©s et plus faciles √† maintenir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L7-w2SPQHEjG"
      },
      "outputs": [],
      "source": [
        "# D√©claration d'une classe Test avec une m√©thode qui retourne une liste de nombres pairs\n",
        "\n",
        "class Test:\n",
        "    def pair(self):\n",
        "        return [a for a in range(11) if a %2 ==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XEsRjanTHEjH"
      },
      "outputs": [],
      "source": [
        "test = Test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xvI3_dv8HEjI",
        "outputId": "f531e822-bdd4-4ce3-b4f0-9139df6947db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " 'pair']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dir(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DLg_NstWHEjI",
        "outputId": "31b8b0ea-07b0-4db7-92b8-82af8bda1f74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2, 4, 6, 8, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "test.pair()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EeMzDE1HEjJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6E9fwQ8HEjJ"
      },
      "source": [
        "\n",
        "____\n",
        "## 3. Pr√©traitement de texte avec la biblioth√®que NLTK\n",
        "\n",
        "Le pr√©traitement du texte aide √† am√©liorer la qualit√© et la fiabilit√© des r√©sultats des analyses NLP en nettoyant et en normalisant les donn√©es textuelles. Cela peut √©galement acc√©l√©rer les analyses en r√©duisant la taille des donn√©es √† analyser.\n",
        "\n",
        "Le pr√©traitement du texte est une √©tape importante dans le traitement du langage naturel (NLP) pour plusieurs raisons :\n",
        "1. Conversion de la casse : La conversion de la casse peut √™tre importante pour garantir la coh√©rence des donn√©es et √©viter les probl√®mes li√©s √† la casse dans les algorithmes de NLP.\n",
        "\n",
        "\n",
        "2. Nettoyage de donn√©es : Les donn√©es textuelles brutes peuvent contenir des erreurs, des abr√©viations, des symboles et d'autres caract√©ristiques qui peuvent affecter la qualit√© des r√©sultats des analyses NLP. Le pr√©traitement du texte aide √† nettoyer ces donn√©es en supprimant les caract√®res ind√©sirables, en corrigeant les erreurs et en normalisant les donn√©es.\n",
        "\n",
        "3. Tokenisation : La tokenisation est l'√©tape cruciale de la division du texte en unit√©s plus petites pour une analyse ult√©rieure. La tokenisation peut √™tre utilis√©e pour diviser les donn√©es textuelles en mots, phrases, symboles et autres unit√©s pertinentes.\n",
        "\n",
        "4. Suppression des Stop words : Les stop words peuvent affecter n√©gativement les performances des algorithmes NLP en ajoutant du bruit aux donn√©es. La suppression des stop words aide √† filtrer les donn√©es et √† am√©liorer les r√©sultats de l'analyse.\n",
        "\n",
        "5. Stemming et Lemmatisation : Le stemming et la lemmatisation sont des techniques importantes pour normaliser les mots et les r√©duire √† leur forme de base pour une analyse plus coh√©rente.\n",
        "\n",
        "Nous utiliserons par la suite le vocabulaire suivant :\n",
        "- **Corpus** : Un corpus est un ensemble de documents textuels rassembl√©s en vu d'un traitement. \n",
        "- **Document** : Document : Un document est une unit√© de texte distincte, telle qu'un livre, un article de journal ou une page web. \n",
        "- **Text** : Le texte est un ensemble de mots et de phrases utilis√©s pour communiquer des informations et des id√©es. \n",
        "- **Token** : Un token est une unit√© d'information dans un texte, qui peut √™tre un mot, un symbole, une poctuation ou tout autre √©l√©ment pertinent.\n",
        "- **Vocabulaire** : C'est l'ensemble des tokens individuels pr√©sents dans l'ensemble du corpus.\n",
        "\n",
        "\n",
        "\n",
        "### 3.1 Traitement des chaines de caract√®re\n",
        "\n",
        "- `str.capitalize()` : retourne la premi√®re lettre en majuscule et le reste en minuscule.\n",
        "- `str.upper()` : retourne la cha√Æne de caract√®res en question en majuscule.\n",
        "- `str.lower()` : retourne la cha√Æne de caract√®res en question en minuscule.\n",
        "- `str.count()` : retourne le nombre d'occurrences d'une sous-cha√Æne dans la cha√Æne de caract√®res en question.\n",
        "- `str.find()` : retourne l'index de la premi√®re occurrence d'une sous-cha√Æne dans la cha√Æne de caract√®res en question. Si la sous-cha√Æne n'est pas trouv√©e, la m√©thode retourne -1.\n",
        "- `str.index()` : retourne l'index de la premi√®re occurrence d'une sous-cha√Æne dans la cha√Æne de caract√®res en question. Si la sous-cha√Æne n'est pas trouv√©e, la m√©thode l√®ve une exception ValueError.\n",
        "- `str.replace()` : retourne une nouvelle cha√Æne de caract√®res dans laquelle toutes les occurrences d'une sous-cha√Æne sont remplac√©es par une autre sous-cha√Æne.\n",
        "- `str.split()` : retourne une liste de cha√Ænes de caract√®res s√©par√©es par un s√©parateur sp√©cifi√©. Si le s√©parateur n'est pas sp√©cifi√©, la m√©thode utilise par d√©faut l'espace.\n",
        "- `str.strip()` : renvoie une copie de la cha√Æne sans les espaces en d√©but et fin.\n",
        "- `str.isdigit()` : retourne True si tous les caract√®res de la cha√Æne sont des chiffres, sinon False.\n",
        "- `str.isalpha()` : retourne True si tous les caract√®res de la cha√Æne sont des lettres, sinon False.\n",
        "- `str.isalnum()` : retourne True si tous les caract√®res de la cha√Æne sont des chiffres ou des lettres, sinon Flase.\n",
        "- `' '.join(['a', 'b'])` : retourn la chainne de caract√®re `'a b'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jPUxD6E2HEjK",
        "outputId": "63167f93-5af0-441a-d3c8-44d975de739b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['isspace',\n",
              " 'istitle',\n",
              " 'isupper',\n",
              " 'join',\n",
              " 'ljust',\n",
              " 'lower',\n",
              " 'lstrip',\n",
              " 'maketrans',\n",
              " 'partition',\n",
              " 'replace',\n",
              " 'rfind',\n",
              " 'rindex',\n",
              " 'rjust',\n",
              " 'rpartition',\n",
              " 'rsplit',\n",
              " 'rstrip',\n",
              " 'split',\n",
              " 'splitlines',\n",
              " 'startswith',\n",
              " 'strip',\n",
              " 'swapcase',\n",
              " 'title',\n",
              " 'translate',\n",
              " 'upper',\n",
              " 'zfill']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "text = \"\"\"Text preprocessing helps improve the quality and reliability of NLP analysis results by cleaning and normalizing textual data. And It's amazing !\n",
        "It can also speed up analyzes by reducing the size of the data to be analyzed.\"\"\"\n",
        "\n",
        "dir(text)[-25:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FgyXdlEeHEjK",
        "outputId": "900a9ed6-972b-40ec-ec2a-ef5cab13bcc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Text preprocessing helps improve the quality and reliability of NLP analysis results by cleaning and normalizing textual data. And It's amazing !\",\n",
              " 'It can also speed up analyzes by reducing the size of the data to be analyzed.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "text.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MgdOJq-hHEjL",
        "outputId": "a13c099e-99c5-4744-9cd1-54104e657930",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text',\n",
              " 'preprocessing',\n",
              " 'helps',\n",
              " 'improve',\n",
              " 'the',\n",
              " 'quality',\n",
              " 'and',\n",
              " 'reliability',\n",
              " 'of',\n",
              " 'nlp',\n",
              " 'analysis',\n",
              " 'results',\n",
              " 'by',\n",
              " 'cleaning',\n",
              " 'and',\n",
              " 'normalizing',\n",
              " 'textual',\n",
              " 'data.',\n",
              " 'and',\n",
              " \"it's\",\n",
              " 'amazing',\n",
              " '!',\n",
              " 'it',\n",
              " 'can',\n",
              " 'also',\n",
              " 'speed',\n",
              " 'up',\n",
              " 'analyzes',\n",
              " 'by',\n",
              " 'reducing',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'the',\n",
              " 'data',\n",
              " 'to',\n",
              " 'be',\n",
              " 'analyzed.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "[token.lower() for token in  text.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPeyvxR6HEjL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh-uq2UeHEjL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5flgiYEFHEjL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AX-YHn3GHEjL",
        "outputId": "d41c32ac-a8d2-4a5d-d0e1-9e46abcd467e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Text',\n",
              " 'preprocessing',\n",
              " 'helps',\n",
              " 'improve',\n",
              " 'the',\n",
              " 'quality',\n",
              " 'and',\n",
              " 'reliability',\n",
              " 'of',\n",
              " 'NLP',\n",
              " 'analysis',\n",
              " 'results',\n",
              " 'by',\n",
              " 'cleaning',\n",
              " 'and',\n",
              " 'normalizing',\n",
              " 'textual',\n",
              " 'data.',\n",
              " 'And',\n",
              " \"It's\",\n",
              " 'amazing',\n",
              " '!',\n",
              " 'It',\n",
              " 'can',\n",
              " 'also',\n",
              " 'speed',\n",
              " 'up',\n",
              " 'analyzes',\n",
              " 'by',\n",
              " 'reducing',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'the',\n",
              " 'data',\n",
              " 'to',\n",
              " 'be',\n",
              " 'analyzed.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Utilisation de la m√©thode split() pour s√©parer les mots\n",
        "tokens = text.split()\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8s4tko3UHEjM",
        "outputId": "9f4491d0-ed21-4aa7-b658-5bab0f0829a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Le texte est-il digital : True\n",
            "Le texte est-il uniquement alphabetique : True\n",
            "Le texte est-il en majuscule : text\n",
            "Le texte est-il en minuscule : Text      \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"\n",
        "Le texte est-il digital : {tokens[0].isalnum()}\n",
        "Le texte est-il uniquement alphabetique : {tokens[0].isalpha()}\n",
        "Le texte est-il en majuscule : {tokens[0].lower()}\n",
        "Le texte est-il en minuscule : {tokens[0].capitalize()}      \n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "060sKY77HEjM",
        "outputId": "2d1d2d3b-0a17-419b-8372-bbdff78ca387",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text lower :  text preprocessing helps improve the quality and reliability of nlp analysis results by cleaning and normalizing textual data. and it's amazing !\n",
            "it can also speed up analyzes by reducing the size of the data to be analyzed.\n",
            "\n",
            "Text uper :  TEXT PREPROCESSING HELPS IMPROVE THE QUALITY AND RELIABILITY OF NLP ANALYSIS RESULTS BY CLEANING AND NORMALIZING TEXTUAL DATA. AND IT'S AMAZING !\n",
            "IT CAN ALSO SPEED UP ANALYZES BY REDUCING THE SIZE OF THE DATA TO BE ANALYZED.\n",
            "\n",
            "Text Capital :  Text preprocessing helps improve the quality and reliability of nlp analysis results by cleaning and normalizing textual data. and it's amazing !\n",
            "it can also speed up analyzes by reducing the size of the data to be analyzed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Text lower : ', text.lower())\n",
        "print()\n",
        "print('Text uper : ', text.upper())\n",
        "print()\n",
        "print('Text Capital : ', text.capitalize())\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5sqFSJ3fHEjN",
        "outputId": "1526b563-77af-422e-e8d7-522094194239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequence du token \"the\" :  3 \n",
            "\n",
            "Position du token \"the\" :  33 \n",
            "\n",
            "Position du token \"the\" :  188 \n",
            "\n",
            "Remplacement du token \"the\" :  Text preprocessing helps improve **** quality and reliability of NLP analysis results by cleaning and normalizing textual data. And It's amazing !\n",
            "It can also speed up analyzes by reducing **** size of **** data to be analyzed. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Utilisation de la m√©thode count() pour compter le nombre d'occurence d'un token\n",
        "print('Frequence du token \"the\" : ', text.count('the'), '\\n')\n",
        "\n",
        "# Utilisation de la m√©thode find() pour trouver la position d'un token\n",
        "print('Position du token \"the\" : ', text.find('the'), '\\n')\n",
        "\n",
        "# Utilisation de la m√©thode index() pour trouver la position d'un token\n",
        "print('Position du token \"the\" : ', text.index('the', 50), '\\n')\n",
        "\n",
        "# Utilisation de la m√©thode replace() pour remplacer un token par un autre\n",
        "print('Remplacement du token \"the\" : ', text.replace('the', '****'), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpn39NjaHEjN"
      },
      "source": [
        "### 3.2 Tokenisation\n",
        "\n",
        "<img src='https://miro.medium.com/max/1050/0*EKgminT7W-0R4Iae.png'>\n",
        "\n",
        "La tokenisation est un processus dans le traitement du langage naturel (NLP) qui consiste √† diviser un texte en unit√©s plus petites appel√©es tokens. \n",
        "\n",
        "Les tokens peuvent √™tre des mots, des phrases, des symboles ou des caract√®res. La tokenisation est souvent la premi√®re √©tape dans le traitement des donn√©es textuelles, car elle permet de pr√©parer le texte pour les analyses ult√©rieures telles que la reconnaissance de la signification, la classification, la g√©n√©ration de r√©sum√©s, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0TomjCHsHEjN",
        "outputId": "84271c0a-a1b5-441f-f5c8-711d1e27e5e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Text',\n",
              " 'preprocessing',\n",
              " 'helps',\n",
              " 'improve',\n",
              " 'the',\n",
              " 'quality',\n",
              " 'and',\n",
              " 'reliability',\n",
              " 'of',\n",
              " 'NLP',\n",
              " 'analysis',\n",
              " 'results',\n",
              " 'by',\n",
              " 'cleaning',\n",
              " 'and',\n",
              " 'normalizing',\n",
              " 'textual',\n",
              " 'data.',\n",
              " 'And',\n",
              " \"It's\",\n",
              " 'amazing',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Tokenization avec la m√©thode split\n",
        "document = text.split('\\n')[0]\n",
        "tokens = document.split(' ')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMcfe7oyHEjN"
      },
      "source": [
        "NLTK (Natural Language Toolkit) est une biblioth√®que Python d√©di√©e au traitement du langage naturel (NLP). Il s'agit d'un outil de r√©f√©rence pour les d√©veloppeurs et les chercheurs travaillant dans le domaine du NLP. \n",
        "\n",
        "Cette biblioth√®que fournit une vari√©t√© de fonctionnalit√©s pour les t√¢ches courantes de NLP :\n",
        "-  telles que la tokenisation, \n",
        "- la reconnaissance d'entit√©s nomm√©es, \n",
        "- la g√©n√©ration de textes, l'analyse de sentiments, \n",
        "- la classification de textes,\n",
        "- le support de plusieurs langues dont le fran√ßais\n",
        "- la gestion des `stop words`\n",
        "\n",
        "Installation de la biblioth√®que NLTK : \n",
        "- sur Windows : `pip install nltk`\n",
        "- sur MacOs : `pip3 install nltk`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y_QNjaseHEjO",
        "outputId": "7dcb6f1d-227e-495f-98f7-f29103fa47d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "# Installation de la biblioth√®que √† partir de l'environement Jupyter\n",
        "!pip3 install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdqTLFHKHEjO",
        "outputId": "45274483-d21f-4391-da48-3a664f9c2ed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# T√©l√©chargement des d√©pendance NLTP\n",
        "import nltk\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tys70cVyHEjO"
      },
      "outputs": [],
      "source": [
        "document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDUMUpmVHEjP"
      },
      "outputs": [],
      "source": [
        "# Import de la fonction word_tokenize depuis la biblioth√®que nltk.tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenization avec la fonction word_tokenize sur le document\n",
        "tokens = word_tokenize(document)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duUzibrAHEjP"
      },
      "source": [
        "### 3.3 Traitement des Stop Words\n",
        "\n",
        "\n",
        "Un stop word d√©signe des mots qui sont souvent ignor√©s ou filtr√©s lors de l'analyse de donn√©es textuelles. Les stop words sont consid√©r√©s comme des mots peu informatifs et ne sont g√©n√©ralement pas pris en compte lors de l'analyse de la signification des textes. \n",
        "\n",
        "Ils peuvent √™tre des pr√©positions, des conjonctions et d'autres mots communs qui ne contribuent pas de mani√®re significative √† la signification d'une phrase ou d'un document. \n",
        "\n",
        "La liste des stop words peut varier en fonction de la langue, du domaine et des objectifs de l'analyse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4ArO4ohHEjP"
      },
      "outputs": [],
      "source": [
        "# Import de la liste de stopwords depuis nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# instanciation de la liste stop_words √† partir du module words (english)\n",
        "stop_words = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZecbvOIHEjQ"
      },
      "outputs": [],
      "source": [
        "stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsPN5BCqHEjQ"
      },
      "outputs": [],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXUu94gyHEjQ"
      },
      "outputs": [],
      "source": [
        "# Import de la liste de stopwords depuis nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# instanciation de la liste stop_words √† partir du module words (english)\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "#print('Nombre de stop words : {}'.format(len(stop_words)), '\\n\\n')\n",
        "\n",
        "#print('liste de tokens : ', tokens, '\\n\\n','liste de stop_word : ', stop_words)\n",
        "\n",
        "# Suppression des stop words avec une liste compr√©hension\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y_e6hVuHEjR"
      },
      "source": [
        "‚û°Ô∏è Supprimons ces mots de nos jetons en utilisant NLTK :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN9f1UI1HEjR"
      },
      "outputs": [],
      "source": [
        "# Filtrage des tokens en fonction des stop words \n",
        "tokens_no_stops = [t for t in tokens if t not in stop_words]\n",
        "tokens_no_stops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3JSiGkDHEjR"
      },
      "source": [
        "A noter que le mot : `And` n'ont pas √©t√© supprim√©s car python est sensible √† la casse :\n",
        "\n",
        "`'And' != 'and' `\n",
        "\n",
        "Veillez √† bien mettre en minuscule votre text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wsgN1yOHEjS"
      },
      "outputs": [],
      "source": [
        "# Filtrage des tokens en fonction des stop words et en minuscule par list comprehension\n",
        "tokens_no_stops_lower = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
        "tokens_no_stops_lower"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq5ORvYpHEjS"
      },
      "outputs": [],
      "source": [
        "[4, 7, 9] + ['Y', 7, True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwBMXKZBHEjS"
      },
      "outputs": [],
      "source": [
        "stop_words + [',', '.', '!', '?', '(' , ')']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdECKI99HEjT"
      },
      "outputs": [],
      "source": [
        "' '.join(tokens_no_stops_lower)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcT0RFKVHEjT"
      },
      "outputs": [],
      "source": [
        "# Utilisation de la m√©thode join() pour concat√©ner les tokens\n",
        "' '.join(tokens_no_stops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnpC1HWvHEjT"
      },
      "source": [
        "### 3.4 Stemming & Lemmatization\n",
        "\n",
        "<img src='https://d2mk45aasx86xg.cloudfront.net/difference_between_Stemming_and_lemmatization_8_11zon_452539721d.webp'>\n",
        "\n",
        "**Stemming** et **lemmatisation** sont deux techniques utilis√©es dans le traitement du langage naturel pour r√©duire les mots √† leur forme de base.\n",
        "\n",
        "**Stemming** : Le stemming est une technique pour extraire la racine d'un mot en enlevant les suffixes, les pr√©fixes et autres modifications morphologiques. L'objectif du stemming est de r√©duire les mots √† leur forme de base pour une analyse plus coh√©rente. Par exemple, les mots \"runner\", \"running\", \"ran\" peuvent √™tre r√©duits √† la forme de base \"run\".\n",
        "\n",
        "**Lemmatisation** : La lemmatisation est similaire au stemming, mais elle vise √† produire un lemme ou un mot normalis√©, qui est une forme valide de dictionnaire pour un mot donn√©. La lemmatisation implique une analyse morphologique plus avanc√©e pour d√©terminer la forme correcte d'un mot, en prenant en compte son contexte et sa d√©finition. Par exemple, le mot \"running\" peut √™tre lemmatis√© en \"run\", tandis que le mot \"better\" peut √™tre lemmatis√© en \"good\".\n",
        "\n",
        "En g√©n√©ral, la lemmatisation est consid√©r√©e comme une technique plus pr√©cise que le stemming, mais elle est √©galement plus lente et plus complexe √† impl√©menter. Les deux techniques peuvent √™tre utiles pour normaliser les mots et am√©liorer les r√©sultats des analyses NLP, mais le choix entre les deux d√©pend des besoins sp√©cifiques d'un projet NLP particulier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elvoIYgJHEjT"
      },
      "outputs": [],
      "source": [
        "# Import des modules PorterStemmer et WordNetLemmatizer depuis la biblioth√®que nltk.stem\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Instanciation des objet stemm et lemm\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-Vn3_E3HEjU"
      },
      "outputs": [],
      "source": [
        "print(stemmer.stem('connection'), stemmer.stem('connected'), stemmer.stem('connective'))\n",
        "\n",
        "# but what if the words don't mean the same thing once truncated\n",
        "print(stemmer.stem('meaning'), stemmer.stem('meanness'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVcGtWXVHEjU"
      },
      "outputs": [],
      "source": [
        "#Lemmatization\n",
        "for word in ['was', 'changed','connected', 'meaning', 'changing', \"wording\"]:\n",
        "    print(lemmatizer.lemmatize(word, 'n'), \n",
        "        lemmatizer.lemmatize(word, 'v'), \n",
        "        lemmatizer.lemmatize(word, 'a'),\n",
        "        lemmatizer.lemmatize(word, 's'),\n",
        "        lemmatizer.lemmatize(word, 'r'))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEXSAPMqHEjU"
      },
      "source": [
        "\n",
        "\n",
        "### 3.5 Ngrammes\n",
        "\n",
        "Jusqu'√† pr√©sent, nous n'avons utilis√© que des unigrammes de mots. Mais parfois, on peut vouloir utiliser aussi des bigrammes , voire des trigrammes de mots. Ou m√™me des unigrammes, des bigrammes et des trigrammes de caract√®res, pourquoi pas ?\n",
        "\n",
        "üëâüèª Voyons sur un exemple simple ce que seraient des bigrammes de la phrase suivante : ¬´ √ätre ou ne pas √™tre ¬ª"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV7rnjx3HEjV"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "# Print the bigrams and trigrams\n",
        "print(tokens, '\\n')\n",
        "\n",
        "print('bigrams:', list(ngrams(tokens, 2)), '\\n')\n",
        "\n",
        "print('trigrams:', list(ngrams(tokens, 3)), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCAoZ0AOHEjV"
      },
      "source": [
        "---\n",
        "\n",
        "# Exercices\n",
        "\n",
        "1. Cre√©ez une classe `Processing` contenant une m√©thode `tokenization` qui tranfome un document en liste de token.\n",
        "Cette m√©thode poss√®de 3 arguments :\n",
        "- `document : str`  --> Le document sous forme de str,\n",
        "- `stem:bool=False` --> Si True, la m√©thode applique la transformation stemming,\n",
        "- `lem:bool=False` --> Si True, la m√©thode applique la transformation lemmatization.\n",
        "\n",
        "La m√©thode garde dans un attribut data, l'ensemble des pr√©c√©dents traitement : le document et la liste des tokens.\n",
        "\n",
        "La m√©thode retourne la liste de token.\n",
        "\n",
        "\n",
        "2. Testez votre programme sur le jeu de donn√©es [suivant](https://drive.google.com/file/d/1FtZFYs41tdhbHTci-0k8yVT3Sn-jYBHh/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QprFG_QHEjV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus = pd.read_csv('coldplay.csv').Lyrics\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ljphypqHEjV"
      },
      "outputs": [],
      "source": [
        "# Cr√©ation d'une classe de pr√©traitement\n",
        "# Import de la fonction word_tokenize depuis la biblioth√®que nltk.tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "class Processing():\n",
        "    def tokenization(self, document, stem:bool=False, lemm:bool=False):\n",
        "        # Instanciation des objet stemm et lemm\n",
        "        stemmer = PorterStemmer()\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        # Tokenization avec la fonction word_tokenize sur le document\n",
        "        document = document.lower()\n",
        "        tokens = word_tokenize(document)\n",
        "\n",
        "        # instanciation de la liste stop_words √† partir du module words (english)\n",
        "        stop_words = stopwords.words('english') + [',', '.', '!', ]\n",
        "\n",
        "        # Suppression des stop words avec une liste compr√©hension\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "        # Stemming\n",
        "        if stem:\n",
        "            tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        # Lemmatization\n",
        "        if lemm:\n",
        "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpFuy8YAHEjW"
      },
      "outputs": [],
      "source": [
        "process = Processing()\n",
        "process.tokenization(corpus[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MutfGBTjHEjW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmSgZQicHEjW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}